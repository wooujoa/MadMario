"""
Best Model í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
1. í•™ìŠµ ì™„ë£Œ í›„ ì‹¤í–‰
2. best_model.chkptë¥¼ ë¡œë“œí•˜ì—¬ í‰ê°€
3. 10 ì—í”¼ì†Œë“œ ì‹¤í–‰ í›„ í†µê³„ ì¶œë ¥
"""

from pathlib import Path
import torch
import numpy as np
from agent import Mario
from wrappers import create_mario_env

print("="*80)
print("ğŸ† Best Model í‰ê°€")
print("="*80)

# ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì„ íƒ
checkpoint_dir = input("ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: checkpoints/2025-12-04T12-30-00): ")
save_dir = Path(checkpoint_dir)

if not save_dir.exists():
    print(f"âŒ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {save_dir}")
    exit(1)

# Best model ê²½ë¡œ
best_model_path = save_dir / 'best_model.chkpt'

if not best_model_path.exists():
    print(f"âŒ best_model.chkptë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {best_model_path}")
    print("í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ê±°ë‚˜ Episode 100 ì´ìƒ í•™ìŠµí•˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    exit(1)

print(f"\nâœ… Best model ë°œê²¬: {best_model_path}")

# í™˜ê²½ ìƒì„±
env = create_mario_env(skip_frame=4)

# Agent ìƒì„± ë° best model ë¡œë“œ
mario = Mario(
    state_dim=(4, 84, 84),
    action_dim=env.action_space.n,
    save_dir=save_dir,
    checkpoint=best_model_path
)

print(f"\nğŸ“Š Best model ì •ë³´:")
print(f"   Best mean reward: {mario.best_mean_reward:.1f}")
print(f"   Exploration rate: {mario.exploration_rate:.4f}")

# Epsilonì„ 0ìœ¼ë¡œ ì„¤ì • (ìˆœìˆ˜ exploitation)
mario.exploration_rate = 0.0
print(f"   í‰ê°€ ëª¨ë“œ: Epsilon = 0.0 (exploitation only)")

# í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜
num_episodes = 10
print(f"\nğŸ® {num_episodes} ì—í”¼ì†Œë“œ í‰ê°€ ì‹œì‘...")
print("-" * 80)

# í‰ê°€ ì‹¤í–‰
episode_rewards = []
episode_steps = []
flag_gets = 0

for ep in range(num_episodes):
    state = env.reset()
    episode_reward = 0
    steps = 0
    
    while True:
        # Action ì„ íƒ (exploitation only)
        action = mario.act(state)
        
        # Step
        next_state, reward, done, info = env.step(action)
        
        episode_reward += reward
        steps += 1
        state = next_state
        
        # ì¢…ë£Œ ì¡°ê±´
        if done or info['flag_get']:
            if info['flag_get']:
                flag_gets += 1
            break
    
    episode_rewards.append(episode_reward)
    episode_steps.append(steps)
    
    flag_icon = "ğŸš©" if info.get('flag_get', False) else "  "
    print(f"Episode {ep+1:2d}: Reward = {episode_reward:6.1f}, Steps = {steps:4d} {flag_icon}")

# í†µê³„ ê³„ì‚°
mean_reward = np.mean(episode_rewards)
std_reward = np.std(episode_rewards)
max_reward = np.max(episode_rewards)
min_reward = np.min(episode_rewards)
mean_steps = np.mean(episode_steps)

print("-" * 80)
print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼:")
print(f"   í‰ê·  ë³´ìƒ: {mean_reward:.1f} Â± {std_reward:.1f}")
print(f"   ìµœê³  ë³´ìƒ: {max_reward:.1f}")
print(f"   ìµœì € ë³´ìƒ: {min_reward:.1f}")
print(f"   í‰ê·  ìŠ¤í…: {mean_steps:.1f}")
print(f"   ê¹ƒë°œ ë„ë‹¬: {flag_gets}/{num_episodes} ({flag_gets/num_episodes*100:.1f}%)")

# í•™ìŠµ ì‹œ best mean rewardì™€ ë¹„êµ
print(f"\nğŸ† ë¹„êµ:")
print(f"   í•™ìŠµ ì‹œ best mean reward: {mario.best_mean_reward:.1f}")
print(f"   í‰ê°€ í‰ê·  ë³´ìƒ:           {mean_reward:.1f}")
diff = mean_reward - mario.best_mean_reward
print(f"   ì°¨ì´:                     {diff:+.1f}")

if abs(diff) < 50:
    print("   âœ… í•™ìŠµ ì‹œ ì„±ëŠ¥ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.")
elif diff > 0:
    print("   ğŸ‰ í‰ê°€ ì„±ëŠ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤! (ìš´ì´ ì¢‹ì•˜ì„ ìˆ˜ ìˆìŒ)")
else:
    print("   âš ï¸  í‰ê°€ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. (ìƒ˜í”Œ ìˆ˜ê°€ ì ì–´ì„œ ê·¸ëŸ´ ìˆ˜ ìˆìŒ)")

print("\n" + "="*80)
print("âœ… í‰ê°€ ì™„ë£Œ!")
print("="*80)

env.close()