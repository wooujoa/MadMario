import random, datetime
from pathlib import Path
import torch

from metrics import MetricLogger
from agent import Mario
from wrappers import create_mario_vec_env   # âœ… ë³‘ë ¬ í™˜ê²½ ìƒì„± í•¨ìˆ˜

# -----------------------------
# 1. í™˜ê²½ / ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
# -----------------------------

NUM_ENVS = 4          # ë³‘ë ¬ë¡œ ëŒë¦´ env ê°œìˆ˜
SKIP_FRAME = 4

env = create_mario_vec_env(num_envs=NUM_ENVS, skip_frame=SKIP_FRAME)

save_dir = Path("checkpoints") / datetime.datetime.now().strftime(
    "%Y-%m-%dT%H-%M-%S"
)
save_dir.mkdir(parents=True, exist_ok=True)

mario = Mario(
    state_dim=env.single_observation_space.shape,   # (4, 84, 84)
    action_dim=env.single_action_space.n,           # Discrete(n)
    save_dir=save_dir,
)

logger = MetricLogger(save_dir)

# í•™ìŠµí•  ì´ ì—í”¼ì†Œë“œ ìˆ˜ (ëª¨ë“  env í•©ì‚° ê¸°ì¤€)
target_episodes = 28000

print(f"\nğŸ¯ í•™ìŠµ ëª©í‘œ: {target_episodes:,} ì—í”¼ì†Œë“œ (ë³‘ë ¬ env: {NUM_ENVS})")
print("-" * 80)

# -----------------------------
# 2. ë³‘ë ¬ í•™ìŠµ ë£¨í”„
# -----------------------------

# ê° envë³„ ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ, ì™„ë£Œ íšŸìˆ˜ ì¶”ì 
episode_rewards = [0.0 for _ in range(NUM_ENVS)]
episode_counts = [0 for _ in range(NUM_ENVS)]
total_episodes = 0

# ë²¡í„° í™˜ê²½ ì´ˆê¸° reset
state = env.reset()  # shape: (NUM_ENVS, 4, 84, 84)

last_logged_episode = 0

while total_episodes < target_episodes:
    # 1) ëª¨ë“  envì— ëŒ€í•´ ì•¡ì…˜ ì„ íƒ (ë°°ì¹˜)
    actions = mario.act_batch(state)  # (NUM_ENVS,)

    # 2) ë²¡í„° env step
    next_state, rewards, dones, infos = env.step(actions)
    # rewards, dones: shape (NUM_ENVS,)
    # infos: ê¸¸ì´ê°€ NUM_ENVSì¸ ë¦¬ìŠ¤íŠ¸(dict)

    # 3) ê° envë³„ë¡œ transition ì €ì¥ / í•™ìŠµ / ë¡œê¹…
    for i in range(NUM_ENVS):
        s = state[i]
        ns = next_state[i]
        a = int(actions[i])
        r = float(rewards[i])
        d = bool(dones[i])
        info = infos[i]

        mario.cache(s, ns, a, r, d)
        q, loss = mario.learn()
        logger.log_step(r, loss, q)
        episode_rewards[i] += r

        # í•´ë‹¹ envì˜ ì—í”¼ì†Œë“œê°€ ëë‚œ ê²½ìš°
        if d or info.get("flag_get", False):
            total_episodes += 1
            episode_counts[i] += 1

            logger.log_episode()
            mario.episode_finished()
            episode_rewards[i] = 0.0

            # AsyncVectorEnvëŠ” done=Trueì¸ envë¥¼ ìë™ resetí•´ì„œ
            # ë‹¤ìŒ obsë¥¼ next_state[i]ë¡œ ë„£ì–´ì£¼ëŠ” êµ¬í˜„ì´ ë§ë‹¤.
            # ë³„ë„ resetì´ í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ env.reset_at(i) ë“±ì„ í˜¸ì¶œ.

    # 4) ë‹¤ìŒ step ì¤€ë¹„
    state = next_state

    # 5) ì£¼ê¸°ì ìœ¼ë¡œ ê¸°ë¡
    if total_episodes >= last_logged_episode + 20:
        logger.record(
            episode=total_episodes,
            epsilon=mario.exploration_rate,
            step=mario.curr_step,
        )
        last_logged_episode = total_episodes

env.close()
print(f"\nâœ… ì´ {total_episodes:,} ì—í”¼ì†Œë“œ í•™ìŠµ ì™„ë£Œ! (ë³‘ë ¬ env: {NUM_ENVS})")
