import random, datetime
from pathlib import Path
import torch

from metrics import MetricLogger
from agent import Mario  # agent_fast_learning.pyë¥¼ agent.pyë¡œ ë³µì‚¬
from wrappers import create_mario_env

env = create_mario_env(skip_frame=4)
save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')
save_dir.mkdir(parents=True)

mario = Mario(
    state_dim=(4, 84, 84), 
    action_dim=env.action_space.n,
    save_dir=save_dir
)

logger = MetricLogger(save_dir)

episodes = 28000  # ë¹ ë¥¸ ê²€ì¦

print(f"\nğŸ¯ í•™ìŠµ ëª©í‘œ: {episodes:,} ì—í”¼ì†Œë“œ")
if episodes == 20000:
    print(f"   ì˜ˆìƒ ì‹œê°„: ~7ì‹œê°„")
    print(f"   ì˜ˆìƒ ë¦¬ì›Œë“œ: ~1900")
elif episodes == 40000:
    print(f"   ì˜ˆìƒ ì‹œê°„: ~13-14ì‹œê°„")
    print(f"   ì˜ˆìƒ ë¦¬ì›Œë“œ: ~2200")
print("-" * 80)

for e in range(episodes):
    state = env.reset()
    
    while True:
        action = mario.act(state)
        next_state, reward, done, info = env.step(action)
        mario.cache(state, next_state, action, reward, done)
        q, loss = mario.learn()
        logger.log_step(reward, loss, q)
        state = next_state
        
        if done or info['flag_get']:
            break
    
    logger.log_episode()
    mario.episode_finished()
    
    if e % 20 == 0:
        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)

env.close()
print(f"\nâœ… {episodes:,} ì—í”¼ì†Œë“œ í•™ìŠµ ì™„ë£Œ!")